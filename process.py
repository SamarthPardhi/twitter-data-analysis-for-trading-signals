"""
process.py: Cleans and processes the raw data from the scraper.

This script is updated to:
- Automatically find the latest CSV file generated by scrapper.py.
- Handle the specific data formats (string-based lists, string timestamps).
- Clean the text content for NLP analysis.
- Save the processed data in Parquet format.
"""
import pandas as pd
import logging
import re
from pathlib import Path

# --- Configuration ---
RAW_DATA_DIR = Path('.')  # Assumes the script runs in the same directory as the CSVs
PROCESSED_DATA_FILE = RAW_DATA_DIR / 'processed_tweets.parquet'
CSV_PATTERN = "stock_market_tweets_*.csv"

# --- Setup Logging ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def find_latest_csv(directory: Path, pattern: str) -> Path | None:
    """Finds the most recently created file in a directory matching a pattern."""
    try:
        latest_file = max(directory.glob(pattern), key=lambda p: p.stat().st_ctime)
        return latest_file
    except ValueError:
        return None

def clean_tweet_content(text: str) -> str:
    """Cleans the text content of a tweet for NLP tasks."""
    if not isinstance(text, str):
        return ""
    text = text.lower()  # Lowercase
    text = re.sub(r'https?://\S+|www\.\S+', '', text)  # Remove URLs
    text = re.sub(r'@[a-zA-Z0-9_]+', '', text)       # Remove mentions
    text = re.sub(r'#[a-zA-Z0-9_]+', '', text)       # Remove hashtags
    text = re.sub(r'[^a-z0-9\s]', '', text)          # Remove punctuation
    text = ' '.join(text.split())                   # Remove extra whitespace
    return text

def main():
    """Main data processing pipeline."""
    logging.info("Starting data processing script...")
    
    # 1. Find the latest raw data file
    latest_csv = find_latest_csv(RAW_DATA_DIR, CSV_PATTERN)
    if not latest_csv:
        logging.error(f"No CSV files found matching pattern '{CSV_PATTERN}' in {RAW_DATA_DIR}. Please run scrapper.py first.")
        return
    logging.info(f"Processing latest raw data file: {latest_csv}")

    # 2. Load the data
    try:
        df = pd.read_csv(latest_csv)
        logging.info(f"Loaded {len(df)} raw tweets for processing.")
    except Exception as e:
        logging.error(f"Could not read the CSV file: {e}")
        return

    # 3. Data Cleaning and Type Conversion
    logging.info("Cleaning data and converting types...")
    
    # Convert timestamp string back to datetime objects
    df['timestamp_utc'] = pd.to_datetime(df['timestamp_utc'])
    
    # Convert comma-separated strings back to lists
    df['hashtags'] = df['hashtags'].apply(lambda x: x.split(', ') if isinstance(x, str) and x else [])
    df['mentions'] = df['mentions'].apply(lambda x: x.split(', ') if isinstance(x, str) and x else [])
    
    # Ensure engagement metrics are numeric
    for col in ['likes', 'retweets', 'replies']:
        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)
        
    # Create a cleaned content column for NLP
    df['cleaned_content'] = df['content'].apply(clean_tweet_content)

    # 4. Deduplication
    # The scraper's tweet_id is synthetic; a better approach is to drop content duplicates
    initial_count = len(df)
    df.drop_duplicates(subset=['username', 'content'], keep='first', inplace=True)
    final_count = len(df)
    logging.info(f"Deduplication removed {initial_count - final_count} tweets.")

    # 5. Store in Parquet format
    try:
        df.to_parquet(PROCESSED_DATA_FILE, engine='pyarrow', index=False)
        logging.info(f"Successfully processed and saved {len(df)} tweets to {PROCESSED_DATA_FILE}")
    except Exception as e:
        logging.error(f"Failed to save data to Parquet file: {e}")

if __name__ == "__main__":
    main()